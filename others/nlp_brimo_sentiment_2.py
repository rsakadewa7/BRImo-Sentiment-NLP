# -*- coding: utf-8 -*-
"""NLP_BRImo_Sentiment_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pRcSoDXomgK-f22ayKEnm5ZHl3fjrHIy

# I. Import Library
"""

!pip install tensorflow-addons

!pip install -U tensorflow==2.11.0

# Required installations
!pip install Sastrawi
!pip install datasets
!pip install transformers
!pip install kagglehub

import pandas as pd
import numpy as np

import kagglehub
from collections import Counter

import re
import nltk
from nltk.corpus import stopwords
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import f1_score

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout, Layer
from tensorflow.keras.models import Model
from tensorflow.keras.regularizers import l2
# from tensorflow.keras.metrics import Recall
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras import backend as K
from tensorflow_addons.optimizers import AdamW

from transformers import AutoTokenizer, TFAutoModel

# Download required NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

"""# II. Data Loading"""

# Load the dataset
path = kagglehub.dataset_download("dannytheodore/brimo-app-review")
dataset_path = f"{path}/brimo_googleplaystore_review.csv"
df = pd.read_csv(dataset_path, index_col=0)

"""# III. Exploratory Data Analysis (EDA)"""

def word_frequency_table(df, column_name, top_n=10):
    # Flatten the text in the column and split into words
    words = ' '.join(df[column_name].dropna()).lower().split()

    # Count the frequency of each word
    word_counts = Counter(words)

    # Create a DataFrame from the word counts
    word_table = pd.DataFrame(word_counts.items(), columns=['Word', 'Frequency'])

    # Sort the DataFrame by frequency in descending order
    word_table = word_table.sort_values(by='Frequency', ascending=False).reset_index(drop=True)

    # Return the top N words (default is 10)
    return word_table.head(top_n)

word_frequency_table(df, 'content', top_n=100)

"""# IV. Feature Engineering (FE)"""

# Text preprocessing function
def preprocess_text(text, stop_words, stemmer):
    try:
        text = text.lower()
        text = re.sub("@[A-Za-z0-9_]+", " ", text)  # Remove mentions
        text = re.sub("#[A-Za-z0-9_]+", " ", text)  # Remove hashtags
        text = re.sub(r"\\n", " ", text)  # Remove newlines
        text = re.sub(r"http\S+", " ", text)  # Remove URLs
        text = re.sub(r"www.\S+", " ", text)  # Remove www URLs
        text = re.sub("[^A-Za-z\s']", " ", text)  # Remove non-letter characters
        tokens = text.split()
        tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords
        tokens = [stemmer.stem(word) for word in tokens]  # Apply stemming
        return ' '.join(tokens)
    except Exception as e:
        print(f"Error processing text: {text}\n{e}")
        return text

# Load IndoBERT Tokenizer and Model
tokenizer = AutoTokenizer.from_pretrained("indobenchmark/indobert-base-p1")
base_model = TFAutoModel.from_pretrained("indobenchmark/indobert-base-p1")

# Custom Keras Layer to wrap the TFAutoModel
class BertLayer(Layer):
    def __init__(self, base_model, **kwargs):
        super(BertLayer, self).__init__(**kwargs)
        self.base_model = base_model

    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state

    def get_config(self):
        config = super(BertLayer, self).get_config()
        config.update({"base_model": self.base_model})
        return config

# Add Pooler Layer (from the first [CLS] token)
class PoolerLayer(Layer):
    def __init__(self, **kwargs):
        super(PoolerLayer, self).__init__(**kwargs)

    def call(self, inputs):
        cls_token = inputs[:, 0, :]  # First token's output (the [CLS] token)
        pooled_output = tf.keras.activations.tanh(cls_token)  # Apply tanh activation
        return pooled_output

# Map the labels to positive, neutral, negative
def map_labels(score):
    if score >= 4:
        return 2  # Positive
    elif score == 3:
        return 1  # Neutral
    else:
        return 0  # Negative

df['label'] = df['score'].apply(map_labels)

# Clean the text data
manual_stopwords = ["di", "ke", "dari", "yang", "dan", "atau", "dengan", "untuk", "ini", "itu", "aja", "saja", "lah", "bri", "brimo", "aplikasi", "rekening", "coba", "yg", "ke", "untuk", "nya", "saya", "dia", "dan", "sangat", "video", "login", "apk", "jadi", "akun", "malah", "uang", "banget", "dalam", "atm", "padahal"]
stop_words = set(stopwords.words('indonesian'))
stop_words.update(manual_stopwords)
factory = StemmerFactory()
stemmer = factory.create_stemmer()
cleaned_texts = [preprocess_text(text, stop_words, stemmer) for text in df['content'].tolist()]

# One-Hot Encoding for Labels
labels = df['label'].tolist()
encoder = OneHotEncoder(sparse_output=False)
labels_one_hot = encoder.fit_transform(np.array(labels).reshape(-1, 1))

# Split the data into training, validation, and test sets (80% train, 10% validation, 10% test)
train_texts, temp_texts, train_labels, temp_labels = train_test_split(cleaned_texts, labels_one_hot, test_size=0.2, random_state=42)
val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5, random_state=42)

# Tokenize the training, validation, and test texts
def tokenize_texts(texts, tokenizer, max_length=128):
    return tokenizer(texts, padding='max_length', truncation=True, max_length=max_length, return_tensors='tf')

train_tokenized_inputs = tokenize_texts(train_texts, tokenizer)
val_tokenized_inputs = tokenize_texts(val_texts, tokenizer)
test_tokenized_inputs = tokenize_texts(test_texts, tokenizer)

train_input_ids = train_tokenized_inputs['input_ids']
train_attention_masks = train_tokenized_inputs['attention_mask']
val_input_ids = val_tokenized_inputs['input_ids']
val_attention_masks = val_tokenized_inputs['attention_mask']
test_input_ids = test_tokenized_inputs['input_ids']
test_attention_masks = test_tokenized_inputs['attention_mask']

# Compute class weights for the model
class_weights = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(labels),
    y=labels
)
class_weights_dict = {i: class_weights[i] for i in range(len(class_weights))}

# Convert datasets to TensorFlow datasets
train_data_tf = tf.data.Dataset.from_tensor_slices(({
    'input_ids': train_input_ids,
    'attention_mask': train_attention_masks
}, train_labels)).batch(32).shuffle(1000)

val_data_tf = tf.data.Dataset.from_tensor_slices(({
    'input_ids': val_input_ids,
    'attention_mask': val_attention_masks
}, val_labels)).batch(32)

test_data_tf = tf.data.Dataset.from_tensor_slices(({
    'input_ids': test_input_ids,
    'attention_mask': test_attention_masks
}, test_labels)).batch(32)

"""# V. Model Definition"""

# Define the model architecture
input_ids = Input(shape=(128,), dtype=tf.int32, name='input_ids')
attention_mask = Input(shape=(128,), dtype=tf.int32, name='attention_mask')

# Use the custom BertLayer
embeddings = BertLayer(base_model)([input_ids, attention_mask])

# Add Pooler Layer to get the [CLS] token output
pooled_output = PoolerLayer()(embeddings)

# Add a dropout layer for regularization
dropout = Dropout(0.4)(pooled_output)

# Add a dense layer with L2 regularization
classifier = Dense(3, activation='softmax', kernel_regularizer=l2(0.01))(dropout)

# Define the complete model
model = Model(inputs=[input_ids, attention_mask], outputs=classifier)

# Freeze BERT layers
for layer in base_model.layers:
    layer.trainable = False  # Freeze BERT model layers

# Make the classifier layer trainable
for layer in model.layers:
    if isinstance(layer, Dense):  # Or any other specific layer type you want to fine-tune
        layer.trainable = True

class F1Score(tf.keras.metrics.Metric):
    def __init__(self, name="f1_score", **kwargs):
        super(F1Score, self).__init__(name=name, **kwargs)
        self.true_positives = self.add_weight(name="tp", initializer="zeros")
        self.false_positives = self.add_weight(name="fp", initializer="zeros")
        self.false_negatives = self.add_weight(name="fn", initializer="zeros")

    def update_state(self, y_true, y_pred, sample_weight=None):
        # Convert probabilities to predicted class indices
        y_pred = tf.argmax(y_pred, axis=-1)
        y_true = tf.argmax(y_true, axis=-1)

        # Calculate true positives, false positives, and false negatives
        tp = tf.reduce_sum(tf.cast((y_true == y_pred) & (y_true != 0), tf.float32))
        fp = tf.reduce_sum(tf.cast((y_true != y_pred) & (y_pred != 0), tf.float32))
        fn = tf.reduce_sum(tf.cast((y_true != y_pred) & (y_true != 0), tf.float32))

        # Update state variables
        self.true_positives.assign_add(tp)
        self.false_positives.assign_add(fp)
        self.false_negatives.assign_add(fn)

    def result(self):
        precision = self.true_positives / (
            self.true_positives + self.false_positives + K.epsilon()
        )
        recall = self.true_positives / (
            self.true_positives + self.false_negatives + K.epsilon()
        )
        f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())
        return f1

    def reset_state(self):
        # Reset all state variables
        self.true_positives.assign(0)
        self.false_positives.assign(0)
        self.false_negatives.assign(0)

# Compile the model with AdamW optimizer and Recall metric
optimizer = AdamW(learning_rate=2e-5, weight_decay=1e-5)
model.compile(
    optimizer=optimizer,
    loss='categorical_crossentropy',
    metrics=[F1Score()]  # Add custom F1Score metric
)

"""# VI. Model Training"""

# Model Checkpoint callback to save the best model during training
checkpoint_callback = ModelCheckpoint("best_model.h5",
                                      save_best_only=True,
                                      monitor='val_loss',
                                      mode='min',
                                      verbose=1)

# Define the EarlyStopping callback
early_stopping = EarlyStopping(
    patience=3,          # Number of epochs with no improvement after which training will be stopped
    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored quantity
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=2)

# Train the model with early stopping
history = model.fit(
    train_data_tf,
    validation_data=val_data_tf,
    epochs=35,
    class_weight=class_weights_dict,
    callbacks=[early_stopping, reduce_lr, checkpoint_callback]
)

# # Evaluate the model on the test data
# test_loss, test_recall = model.evaluate(test_data_tf)
# print(f"Test Loss: {test_loss}")
# print(f"Test Recall: {test_recall}")

"""# VII. Model Evaluation

# VIII. Model Inference

# IX. Model Saving
"""

from google.colab import files
files.download('/content/best_model.h5')

# Save the model in HDF5 format
model.save("final_model.h5")

# Save the model in Keras format
# model.save("final_model.keras")

"""# X. Conclusion

# XI. Further Improvement
"""